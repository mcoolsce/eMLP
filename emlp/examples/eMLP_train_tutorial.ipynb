{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the eMLP\n",
    "\n",
    "In this tutorial, we will train a simple eMLP model on the first 10 molecules of the [eQM7 dataset](https://archive.materialscloud.org/record/2021.154). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "[Extended xyz](https://wiki.fysik.dtu.dk/ase/ase/io/formatoptions.html#extxyz) files are the easiest file format to store and process data for use within the eMLP. In general, a single snapshot should look like this:\n",
    "\n",
    "    10\n",
    "    Properties=species:S:1:pos:R:3:Z:I:1:force:R:3 energy=-1101.1872787241705 efield=\"-0.004991 0.000988 0.002404\"\n",
    "    C\t0.998176\t-0.002580\t-0.004569\t6\t-0.013318\t-0.000720\t-0.000771\n",
    "    H\t2.142527\t-0.002458\t0.004496\t1\t-1.596604\t0.002982\t-0.005142\n",
    "    H\t0.616300\t1.076179\t0.004496\t1\t0.540086\t-1.563694\t-0.004782\n",
    "    H\t0.609230\t-0.552938\t0.920356\t1\t0.552016\t0.805681\t-1.339545\n",
    "    H\t0.624429\t-0.531433\t-0.948150\t1\t0.517872\t0.755750\t1.350292\n",
    "    Es\t0.998192\t-0.002583\t-0.004577\t99\t0.000000\t0.000000\t0.000000\n",
    "    Es\t0.753050\t0.702150\t-0.000837\t99\t0.000000\t0.000000\t0.000000\n",
    "    Es\t1.758252\t-0.003409\t-0.000813\t99\t0.000000\t0.000000\t0.000000\n",
    "    Es\t0.757369\t-0.350852\t-0.626559\t99\t0.000000\t0.000000\t0.000000\n",
    "    Es\t0.748588\t-0.363237\t0.597844\t99\t0.000000\t0.000000\t0.000000\n",
    "\n",
    "This examples describes methane CH<sub>4</sub>, a molecule with 10 electrons or 5 electron pairs, which are stored as *Einsteinium* atoms with atomic number 99 and are just appended to the list of all the other atoms. Note that in this example, the core electron pairs are still included. They should be filtered out later. Electric field information is included via the comment line with the keyword `efield`.  In this tutorial, the data files have already been stored in a respective train and validation set in `data/train.xyz` and `data/validation.xyz`.\n",
    "\n",
    "The first step to train the eMLP, is converting all the data into a Tensorflow Record file (tfr-file). This can be easily done with the `TFRWriter` class of the eMLP. Before we can do so, we need to tell the eMLP what kind of properties to look for in extended xyz files. The is done via the `list_of_properties` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_properties = ['positions', 'numbers', 'centers', 'energy', 'forces', 'efield'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `'centers'` denote the electron pair positions. If the property `'center_forces'` is not given explicitly, they are assumed to be zero (as is the case in this example). Afterwards, we can convert both the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emlp.datasets import TFRWriter\n",
    "\n",
    "writer = TFRWriter('train.tfr', list_of_properties = list_of_properties, reference = 'pbe0_aug-cc-pvtz', filter_centers = True)\n",
    "writer.write_from_xyz('data/train.xyz')\n",
    "writer.close()\n",
    "\n",
    "writer = TFRWriter('validation.tfr', list_of_properties = list_of_properties, reference = 'pbe0_aug-cc-pvtz', filter_centers = True)\n",
    "writer.write_from_xyz('data/validation.xyz')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that eMLP will print out the amount of configurations being stored in the respective tfr files, together with the total number of atoms and some statistics of the energy. The total number of configurations should be remembered as this information is not included into the tfr-files itself (see below when constructing the data sets).\n",
    "\n",
    "The TFRWriter class has several arguments:\n",
    "- `filter_centers`: (boolean) If true, filter the core electron pairs\n",
    "- `reference`: If a number, substract that constant value from all the energies. If a string, use the molecular fragments (H<sub>2</sub>, CH<sub>4</sub>, NH<sub>3</sub> and H<sub>2</sub>O) as a reference at the given level of theory, as described in the [eMLP paper](https://doi.org/10.1021/acs.jctc.1c00978), see eq. 32. So far, only `pbe0_aug-cc-pvtz` is supported.\n",
    "- `per_atom_reference`: a number indicating the reference energy per atomic core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the training procedure\n",
    "\n",
    "First, one has the choose a Tensorflow distributed training strategy (https://www.tensorflow.org/guide/distributed_training#types_of_strategies). This enables multi-GPU training. The `MirroredStrategy` is a good default, which is also valid for single GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the following code should be initialized within the scope of the same strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emlp.longrange import LongrangeCoulomb, LongrangeEwald\n",
    "with strategy.scope():\n",
    "    longrange_compute = LongrangeCoulomb(cutoff = 16.5, sigma = 1.2727922061357857)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longrange_compute argument specifies the algorithm to compute the electrostatic interactions within the eMLP. There are two possible choices. For nonperiodic systems, one should use the `LongrangeCoulomb` class with arguments:\n",
    "- `cutoff`: the maximum distance for which all the pairwise electrostatic interactions are being computed.\n",
    "- `sigma`: the width of all the Gaussian charges of every particle. The default value of 1.272792 A is being used in the eMLP paper.\n",
    "\n",
    "For periodic systems, the `LongrangeEwald` class can be used with the arguments:\n",
    "- `real_space_cancellation`: (boolean) if True, the width of the screening charges take the same value as the width of the Gaussian charges of the particles. In this case, the direct space contribution of the Ewald summation vanishes which increases the computational efficiency. Therefore, we recommend to use this option.\n",
    "- `sigma`: the width of all the Gaussian charges of every particle. The default value of 1.272792 A is being used in the eMLP paper.\n",
    "- `gcut`: the cutoff in reciprocal space. Defaults to 0.25.\n",
    "- `cutoff`: the cutoff in the direct space contribution. Defaults to 12 A. Is ignored when using the `real_space_cancellation` argument.\n",
    "- `alpha`: the alpha parameter in the Ewald summation. Defaults to 4/15. Is ignored when using the `real_space_cancellation` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emlp.datasets import DataSet, DataAugmentation\n",
    "with strategy.scope():\n",
    "    train_data = DataSet(['train.tfr'], num_configs = 4000, cutoff = 4.0, longrange_compute = longrange_compute, batch_size = 64, float_type = 32, num_parallel_calls = 8, \n",
    "                          strategy = strategy, list_of_properties = list_of_properties, augment_data = DataAugmentation())\n",
    "    validation_data = DataSet(['validation.tfr'], num_configs = 1000, cutoff = 4.0, longrange_compute = longrange_compute, batch_size = 64, float_type = 32, num_parallel_calls = 8, \n",
    "                              strategy = strategy, list_of_properties = list_of_properties, test = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the training and validation data sets are initialized via the `DataSet` class starting from a list of tfr files. Here, it is important to specify the correct amount of configurations being stored in the data sets via the `num_configs` argument (see above when generating the tfr-files). Otherwise, the eMLP will not correctly count the number of epochs. The validation set should have the extra argument `test=True`. Other arguments may include:\n",
    "- `cutoff`: The cutoff distance of the shortranged MLP part of the eMLP. Defaults to 4A. This value should not necessarily be equal to the cutoff radius of the longranged part.\n",
    "- `longrange_compute`: the algorithm being used for the longrange electrostatic computation. If `None`, no longrange interactions will be computed.\n",
    "- `batch_size`: the batch size being used while training.\n",
    "- `test`: (boolean) Set True for the validation set, False for the training set.\n",
    "- `num_parallel_calls`: The number of configurations to be preprocessed in parallel. A good default is the number of CPU cores available.\n",
    "- `strategy`: The distributed strategy defined above.\n",
    "- `list_of_properties`: The list of properties defined above.\n",
    "- `augment_data`: The data augmentation instance. If None, no data augmentation is being applied.\n",
    "\n",
    "If data augmentation is necessary, it can be included by creating a `DataAugmentation` instance, which has the following arguments:\n",
    "- `delta_lower`: The minimum displacement of an electron pair. Default is 0.06 A. \n",
    "- `delta_upper`: The minimum displacement of an electron pair. Default is 0.12 A. \n",
    "- `k`: The *k* value in the eMLP paper (eq. 30), tuning the augmentation strength. Defaults to 2.\n",
    "- `percentage`: The percentage of augmentated configurations in every batch. Defaults to 0.1.\n",
    "- `cutoff`: The maximum distance over which a augmentated electron pair influences the forces of the other atoms. Defaults to 4A.\n",
    "- `periodic`: (boolean) Set True, when working with periodic structures. Then, it applies a simple form of the minimum image convention. Defaults to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emlp.schnet import SchNet\n",
    "from emlp.reference import ConsistentFragmentsReference\n",
    "with strategy.scope():\n",
    "    model = SchNet(cutoff = 4., n_max = 32, num_layers = 4, start = 0.0, end = 4.0, num_filters = 128, num_features = 512, shared_W_interactions = False, float_type = 32, \n",
    "                   cutoff_transition_width = 0.5, reference = ConsistentFragmentsReference('pbe0_aug-cc-pvtz'), longrange_compute = longrange_compute)\n",
    "    #model = SchNet.from_restore_file('model_dir/model_name_2.00', reference = ConsistentFragmentsReference('pbe0_aug-cc-pvtz'), longrange_compute = longrange_compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the architecture of the MLP should specified. Here, we choose for the SchNet architecture but, in principle, any MLP is valid. One can tune the following arguments:\n",
    "- `cutoff`: The cutoff of the shortranged MLP. Should take the same value as the cutoff specified in the `DataSet` class.\n",
    "- `n_max`: The number of radial features.\n",
    "- `num_features`: The number of features for every particle.\n",
    "- `num_filters`: The number of filters. \n",
    "- `num_layers`: The number of layers or interaction blocks\n",
    "- `start`: The distance of the first radial feature.\n",
    "- `end`: The distance of the last radial feature.\n",
    "- `cutoff_transition_width`: The transition width of the cutoff function\n",
    "- `shared_W_interactions`: Whether or not the share the same filter functions for every interaction block. Defaults to False.\n",
    "- `longrange_compute`: the algorithm being used for the longrange electrostatic computation. If None, no longrange interactions will be computed.\n",
    "- `reference`: If a number, use a constant reference while training. Otherwise, a reference instance can be given here to compute the reference energy (see below).\n",
    "- `xla`: (boolean). Whether or not to use [XLA](https://www.tensorflow.org/xla) when training the model. Defaults to False.\n",
    "\n",
    "The reference instance can be one of the following:\n",
    "- `ConstantReference(value = 0., per_atom = False)`: a constant reference energy (per atom when specified).\n",
    "- `ConsistentFragmentsReference(label = 'pbe0_aug-cc-pvtz')`: molecular fragments are being used for the reference energy. See eq. 32 in the eMLP paper. Here, they are computed on the fly in every batch.\n",
    "- `ConstantFragmentsReference(label = 'pbe0_aug-cc-pvtz')`: molecular fragments are being used for the reference energy. See eq. 32 in the eMLP paper. Here, they have a constant value.\n",
    "\n",
    "When restarting from a pretrained model, or simply when resuming the training procedure, the MLP can be loaded as follows: \n",
    "\n",
    "    model = SchNet.from_restore_file('model_dir/model_name_2.00', reference = ConsistentFragmentsReference('pbe0_aug-cc-pvtz'), longrange_compute = longrange_compute) \n",
    "where `'model_dir/model_name_2.00'` is the location where the MLP is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emlp.learning_rate_manager import ExponentialDecayLearningRate\n",
    "with strategy.scope():\n",
    "    optimizer = tf.optimizers.Adam(3e-04)\n",
    "    learning_rate_manager = ExponentialDecayLearningRate(initial_learning_rate = 3e-04, decay_rate = 0.5, decay_epochs = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the optimizer and learning rate schedular are being loaded. Any tensorflow optimizer can be used here (https://www.tensorflow.org/api_docs/python/tf/keras/optimizers ). Two popular learning rate schedulars can be used:\n",
    "- `ExponentialDecayLearningRate(initial_learning_rate = 3e-04, decay_rate = 0.5, decay_epochs = 300)`: An exponentially decaying learning rate. The learning rate starts at the initial value of `initial_learning_rate` and decays every `decay_epochs` by a factor `decay_rate`.\n",
    "- `ConstantDecayLearningRate(initial_learning_rate = 1e-04, decay_factor = 0.5, min_learning_rate = 1e-07, decay_patience = 25)`: Starting from an initial learning rate of `initial_learning_rate`, the learning rate decays with a factor of `decay_factor` when the validation losses have not decreased anymore after `decay_patience` epochs. The training stops when the learning rate has dropped below `min_learning_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emlp.losses import MSE, MAE\n",
    "with strategy.scope():\n",
    "    losses = [MSE('energy', scale_factor = 1., per_atom = True), MSE('forces', scale_factor = 1.), MSE('center_forces', scale_factor = 1.)]\n",
    "    validation_losses = [MAE('energy', per_atom = True), MAE('forces', scale_factor = 1.), MAE('center_forces', scale_factor = 1.)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the training and validation losses, one can use mean squared errors (MSE) or mean absolute errors (MAE). They should be given as a list, where the `scale_factor` argument is the weight tuning the relative weights.\n",
    "\n",
    "Finally, the `SaveHook` class specifies the save location and how frequently the model is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emlp.hooks import SaveHook\n",
    "with strategy.scope():\n",
    "    savehook = SaveHook(model, ckpt_name = 'model_dir/model_name', max_to_keep = 5, save_period = 1.0, history_period = 8.0,\n",
    "                        npz_file = 'model_dir/model_name.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following arguments can be specified:\n",
    "- `ckpt_name`: the location of where to save to model (checkpoint files).\n",
    "- `max_to_keep`: How many saves are at most being stored. Older saves will always be deleted.\n",
    "- `save_period`: The amount of epochs after which the validation set losses are being calculated and **if the current validation losses are the lowest**, the model is saved.\n",
    "- `history_period`: The amount of eopchs after which the model is saved irrespective of the current validation losses. Can be switched off by setting the value to `None`.\n",
    "- `npz_file`: All the losses, time per epoch and other information will be stored to produce graphs in an npz file at this location.\n",
    "\n",
    "Finally, at the end, one can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emlp.training import Trainer\n",
    "with strategy.scope():\n",
    "    trainer = Trainer(model, losses, train_data, validation_data, strategy = strategy, optimizer = optimizer, savehook = savehook, \n",
    "                      learning_rate_manager = learning_rate_manager, validation_losses = validation_losses)\n",
    "    trainer.train(verbose = True, validate_first = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNFF",
   "language": "python",
   "name": "nnff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
